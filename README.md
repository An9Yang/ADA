### AI Movie Describer Agent (ADA) 技术文档

**版本：1.0**  
\*\*最后更新日期：2025 年 02 月 23 日

---

## 目录

1. **项目概述**
2. **当前实现（阶段 1）**
   - 2.1 系统架构
   - 2.2 核心模块与技术栈
   - 2.3 工作流程与实现细节
   - 2.4 测试结果与限制
3. **未来开发计划（阶段 2+）**
   - 3.1 实时图像处理优化
   - 3.2 硬件集成与可穿戴设备
   - 3.3 多模态输入与协作功能
4. **愿景与可扩展性**

---

## 1. 项目概述

**目标**：为视障人士提供实时、高精度的电影内容音频描述服务，通过 AI 技术替代传统人工 Movie Describer，实现音画同步率 ≥86%，并支持未来扩展到实时环境感知场景。  
**核心价值**：

- **无障碍化**：通过自动化流程降低人工成本，提升服务覆盖率。
- **低延迟**：端到端处理时间控制在视频播放时间的 1.2 倍以内。
- **可扩展性**：架构设计支持未来硬件集成与多模态输入。

---

## 2. 当前实现（阶段 1）

### 2.1 系统架构

![系统架构图]()

- **前端**：Discord 语音交互界面（支持多语言输入）。
- **中台**：Make.com 自动化流程引擎。
- **后端**：Google Drive（存储）、Google Sheets（结构化数据）、OpenAI API（图像分析与文本生成）、ElevenLabs（TTS）。

### 2.2 核心模块与技术栈

| **模块**           | **技术实现**               | **关键参数**                              |
| ------------------ | -------------------------- | ----------------------------------------- |
| **视频搜索与截屏** | YouTube Data API + FFmpeg  | 截屏频率：2 秒/帧，分辨率：720p           |
| **图像分析**       | OpenAI GPT-4o              | 5 张/组，上下文窗口：1024 tokens          |
| **文本对齐与润色** | OpenAI GPT-o3 mini         | 最大长度：400 字符/片段                   |
| **语音合成**       | ElevenLabs Multilingual v2 | 语音风格：中性叙述，语速：140-180 词/分钟 |
| **数据存储**       | Google Drive + Sheets      | 结构化字段：时间戳、文本片段、音频 URL    |

### 2.3 工作流程与实现细节

#### 步骤 1：用户输入与视频处理

1. 用户通过 Discord 语音指令触发流程（例如：“播放《阿凡达》第 30 分钟至 40 分钟”）。
2. 系统通过 Make.com 调用 YouTube Data API 定位视频，FFmpeg 按 2 秒间隔截屏，存储至 Google Drive（命名规则：`<视频ID>_<时间戳>.png`）。

#### 步骤 2：场景分析与文本生成

1. 每 5 张截图（覆盖 10 秒视频）输入 GPT-4o，生成符合 Movie Describer 风格的描述（示例输出）：
   > "主角身着蓝色战甲，站立在悬浮山脉边缘，背景是发光的森林，镜头缓慢拉远，显示下方深渊。"
2. 结果按时间顺序写入 Google Sheets，字段包括：`起始时间`、`结束时间`、`原始文本`、`状态`。

#### 步骤 3：全局对齐与语音生成

1. GPT-o3 mini 对相邻文本片段进行语义对齐，解决截断问题（例如合并“角色转身”和“门关闭”为连贯动作）。
2. ElevenLabs 生成音频片段，时长严格匹配 10 秒（±0.5 秒误差），存储至 Google Drive 并返回 Discord 播放。

### 2.4 测试结果与限制

| **指标**     | **当前性能**            | **优化方向**                                     |
| ------------ | ----------------------- | ------------------------------------------------ |
| 音画同步率   | 86%                     | 动态调整 TTS 语速算法                            |
| 端到端延迟   | 1.5 倍播放时间          | 并行化图像处理流水线                             |
| 场景连贯性   | 92%准确率               | 引入视频动作预测模型                             |
| **主要限制** | 依赖第三方 API 速率限制 | 部署 realtime 模型（如 GPT 4o/4o mini realtime） |

---

## 3. 未来开发计划（阶段 2+）

### 3.1 实时图像处理优化（2025 Q1-Q2）

- **动态帧率调整**：基于动作复杂度自动调节截屏频率（1-4 秒）。
- **AI 对齐模块**：
  - 训练专用对比学习模型，检测语音与画面的语义偏差（目标：Δ<0.5 秒）。
  - 硬件要求：NVIDIA Jetson Orin（64GB 内存，支持 INT8 量化）。

### 3.2 硬件集成与智能眼镜（2025 Q3-Q4）

| **组件**     | **规格**                             | **功能**                               |
| ------------ | ------------------------------------ | -------------------------------------- |
| 摄像头模块   | Sony IMX678（低光增强）              | 实时环境图像捕捉（120° FOV）           |
| 音频输出     | 骨传导扬声器（防漏音）               | 隐私保护与户外场景兼容                 |
| 本地 AI 芯片 | Qualcomm Snapdragon 8 Gen3 AI Engine | 离线运行轻量化 GPT-4 模型（4bit 量化） |

### 3.3 协作功能扩展（2026+）

- **多用户协同标注**：允许志愿者修正 AI 生成的描述，构建高质量数据集。
- **紧急事件检测**：通过视觉 Agent 识别危险场景（如火灾、跌倒）并触发警报。

---

## 4. 愿景与可扩展性

**终极目标**：打造视障人士的通用感知增强系统，覆盖场景：

- **娱乐**：电影、游戏、剧院实时解说。
- **导航**：结合 AR 眼镜的路径引导（障碍物检测精度 ≥99%）。
- **社交**：人脸识别与情绪分析（“左侧 3 米处是您的朋友，他正在微笑”）。

**技术路线图**：  
![愿景路线图](https://drive.google.com/file/d/1fALAP3Ggj4FICrCk-h9sj3rh77wsY9xe/view?usp=drive_link)

---

**附录**：

- 数据隐私保护方案（符合 GDPR 与 HIPAA 标准）
- 第三方 API 调用成本分析
- 开源代码仓库地址（待发布）

---
